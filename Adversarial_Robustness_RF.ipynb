{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkFECpak3kjbuKADtj6coC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0XRtJaobYOO","executionInfo":{"status":"ok","timestamp":1742136768240,"user_tz":-420,"elapsed":22743,"user":{"displayName":"Viá»‡t Nguyá»…n VÄƒn","userId":"05507288993718818686"}},"outputId":"5b7de067-9661-452f-ae6f-bdc90d019121"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import shap\n","import joblib\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.utils import shuffle\n","\n","# ================================================================\n","# 1. Äá»c vÃ  xá»­ lÃ½ dá»¯ liá»‡u\n","# ================================================================\n","data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset.csv')\n","print(\"Dataset shape:\", data.shape)\n","\n","# Giáº£ sá»­ cá»™t Ä‘áº§u tiÃªn lÃ  nhÃ£n, cÃ¡c cá»™t cÃ²n láº¡i lÃ  cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã£ trÃ­ch xuáº¥t\n","target_col = data.columns[0]\n","X = data.iloc[:, 1:]\n","y = data[target_col]\n","\n","# Náº¿u y lÃ  sá»‘ liÃªn tá»¥c vÃ  cÃ³ nhiá»u giÃ¡ trá»‹, chuyá»ƒn thÃ nh nhÃ£n phÃ¢n loáº¡i dá»±a trÃªn median\n","if np.issubdtype(y.dtype, np.number) and y.nunique() > 10:\n","    threshold = y.median()\n","    y = np.where(y < threshold, 0, 1)\n","\n","# Chia dá»¯ liá»‡u thÃ nh táº­p train vÃ  test (stratify Ä‘á»ƒ giá»¯ tá»‰ lá»‡ nhÃ£n)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.1, random_state=42, stratify=y\n",")\n","\n","# ================================================================\n","# 2. Adversarial Training: Táº¡o dá»¯ liá»‡u Ä‘á»‘i khÃ¡ng tá»« táº­p train\n","# ================================================================\n","# XÃ¡c Ä‘á»‹nh top feature cáº§n thÃªm nhiá»…u. Náº¿u dataset khÃ´ng cÃ³ tÃªn 'average_subdomain_length', 'url_length', 'entropy_of_url'\n","top_features = ['average_subdomain_length', 'url_length', 'entropy_of_url']\n","for f in top_features:\n","    if f not in X.columns:\n","        top_features = list(X.columns[:3])\n","        break\n","print(\"Top features for adversarial training:\", top_features)\n","\n","noise_factor = 0.05  # 5% noise\n","X_train_adv = X_train.copy()\n","for f in top_features:\n","    # ThÃªm nhiá»…u theo phÃ¢n phá»‘i Gaussian\n","    X_train_adv[f] = X_train_adv[f] * (1 + np.random.normal(0, noise_factor, X_train_adv[f].shape))\n","\n","# Káº¿t há»£p dá»¯ liá»‡u gá»‘c vÃ  dá»¯ liá»‡u Ä‘á»‘i khÃ¡ng\n","X_train_aug = pd.concat([X_train, X_train_adv], axis=0)\n","y_train_aug = np.concatenate([y_train, y_train])\n","X_train_aug, y_train_aug = shuffle(X_train_aug, y_train_aug, random_state=42)\n","print(\"Augmented training set shape:\", X_train_aug.shape)\n","\n","# Huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest trÃªn táº­p dá»¯ liá»‡u má»Ÿ rá»™ng\n","rf_adv = RandomForestClassifier(\n","    n_estimators=1000,\n","    max_depth=100,\n","    min_samples_split=2,\n","    min_samples_leaf=1,\n","    max_features='log2',\n","    bootstrap=False,\n","    class_weight='balanced',\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_adv.fit(X_train_aug, y_train_aug)\n","\n","# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh adversarial training trÃªn táº­p test gá»‘c\n","y_pred_adv = rf_adv.predict(X_test)\n","acc_adv = accuracy_score(y_test, y_pred_adv)\n","print(\"\\nğŸ”¥ Adversarial Training Model Accuracy (Original Test): {:.2f}%\".format(acc_adv * 100))\n","print(\"\\nğŸ“Œ Classification Report (Original Test - Adv Model):\\n\", classification_report(y_test, y_pred_adv))\n","print(\"\\nğŸ“Œ Confusion Matrix (Original Test - Adv Model):\\n\", confusion_matrix(y_test, y_pred_adv))\n","\n","# ================================================================\n","# 3. Iterative Feature Re-selection: ÄÃ¡nh giÃ¡ láº¡i táº§m quan trá»ng cá»§a feature báº±ng SHAP\n","# ================================================================\n","explainer_adv = shap.TreeExplainer(rf_adv)\n","# TÃ­nh SHAP values cho táº­p test\n","shap_values_adv = explainer_adv.shap_values(X_test)\n","\n","# TÃ­nh trung bÃ¬nh giÃ¡ trá»‹ SHAP tuyá»‡t Ä‘á»‘i cho tá»«ng feature cá»§a lá»›p 1 (hoáº·c trung bÃ¬nh trÃªn táº¥t cáº£ lá»›p)\n","shap_importance = np.abs(shap_values_adv[1]).mean(axis=0)\n","feature_importance_df = pd.DataFrame({\n","    'feature': X.columns,\n","    'shap_importance': shap_importance\n","}).sort_values(by='shap_importance', ascending=False)\n","print(\"\\nFeature importance from adversarial model (SHAP):\")\n","print(feature_importance_df.head(10))\n","\n","# Chá»n ra cÃ¡c feature cÃ³ robust (vÃ­ dá»¥: cÃ¡c feature cÃ³ SHAP importance trÃªn ngÆ°á»¡ng 0.01)\n","robust_threshold = 0.01\n","robust_features = feature_importance_df[feature_importance_df['shap_importance'] > robust_threshold]['feature'].tolist()\n","print(\"Robust features after re-selection:\", robust_features)\n","\n","# Huáº¥n luyá»‡n láº¡i mÃ´ hÃ¬nh vá»›i táº­p robust features\n","X_train_reduced = X_train[robust_features]\n","X_test_reduced = X_test[robust_features]\n","\n","rf_reduced = RandomForestClassifier(\n","    n_estimators=1000,\n","    max_depth=100,\n","    min_samples_split=2,\n","    min_samples_leaf=1,\n","    max_features='log2',\n","    bootstrap=False,\n","    class_weight='balanced',\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_reduced.fit(X_train_reduced, y_train)\n","\n","y_pred_reduced = rf_reduced.predict(X_test_reduced)\n","acc_reduced = accuracy_score(y_test, y_pred_reduced)\n","print(\"\\nğŸ”¥ Accuracy with robust features only: {:.2f}%\".format(acc_reduced * 100))\n","print(\"\\nğŸ“Œ Classification Report (Robust Features):\\n\", classification_report(y_test, y_pred_reduced))\n","print(\"\\nğŸ“Œ Confusion Matrix (Robust Features):\\n\", confusion_matrix(y_test, y_pred_reduced))\n","\n","# ================================================================\n","# 4. Dynamic Feature Weighting: Äiá»u chá»‰nh trá»ng sá»‘ cÃ¡c feature dá»±a trÃªn SHAP\n","# ================================================================\n","# Láº¥y dictionary trá»ng sá»‘ tá»« SHAP (dá»±a trÃªn feature_importance_df)\n","feature_weights = feature_importance_df.set_index('feature')['shap_importance'].to_dict()\n","\n","# Chuáº©n hÃ³a trá»ng sá»‘\n","total_weight = sum(feature_weights.values())\n","feature_weights_normalized = {f: w / total_weight for f, w in feature_weights.items()}\n","print(\"\\nNormalized feature weights:\")\n","print(feature_weights_normalized)\n","\n","# Táº¡o táº­p dá»¯ liá»‡u weighted báº±ng cÃ¡ch nhÃ¢n má»—i feature vá»›i trá»ng sá»‘ tÆ°Æ¡ng á»©ng\n","X_train_weighted = X_train.copy()\n","X_test_weighted = X_test.copy()\n","for f in X.columns:\n","    if f in feature_weights_normalized:\n","        X_train_weighted[f] = X_train_weighted[f] * feature_weights_normalized[f]\n","        X_test_weighted[f] = X_test_weighted[f] * feature_weights_normalized[f]\n","\n","# Huáº¥n luyá»‡n láº¡i mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u weighted\n","rf_weighted = RandomForestClassifier(\n","    n_estimators=1000,\n","    max_depth=100,\n","    min_samples_split=2,\n","    min_samples_leaf=1,\n","    max_features='log2',\n","    bootstrap=False,\n","    class_weight='balanced',\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_weighted.fit(X_train_weighted, y_train)\n","\n","y_pred_weighted = rf_weighted.predict(X_test_weighted)\n","acc_weighted = accuracy_score(y_test, y_pred_weighted)\n","print(\"\\nğŸ”¥ Accuracy with dynamic feature weighting: {:.2f}%\".format(acc_weighted * 100))\n","print(\"\\nğŸ“Œ Classification Report (Dynamic Weighting):\\n\", classification_report(y_test, y_pred_weighted))\n","print(\"\\nğŸ“Œ Confusion Matrix (Dynamic Weighting):\\n\", confusion_matrix(y_test, y_pred_weighted))\n","\n","# ================================================================\n","# 5. LÆ°u láº¡i mÃ´ hÃ¬nh (cÃ³ thá»ƒ lÆ°u tá»«ng model riÃªng náº¿u cáº§n)\n","# ================================================================\n","joblib.dump(rf_adv, 'rf_adv_model.pkl')\n","joblib.dump(rf_reduced, 'rf_reduced_model.pkl')\n","joblib.dump(rf_weighted, 'rf_weighted_model.pkl')\n","print(\"\\nCÃ¡c model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u thÃ nh cÃ´ng dÆ°á»›i tÃªn: 'rf_adv_model.pkl', 'rf_reduced_model.pkl', vÃ  'rf_weighted_model.pkl'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8hjjatlcBxV","outputId":"b6a3249a-bbc9-4bbe-e460-a8c7659ed9a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset shape: (247950, 42)\n","Top features for adversarial training: ['average_subdomain_length', 'url_length', 'entropy_of_url']\n","Augmented training set shape: (446310, 41)\n","\n","ğŸ”¥ Adversarial Training Model Accuracy (Original Test): 97.17%\n","\n","ğŸ“Œ Classification Report (Original Test - Adv Model):\n","               precision    recall  f1-score   support\n","\n","           0       0.97      0.98      0.97     12854\n","           1       0.97      0.97      0.97     11941\n","\n","    accuracy                           0.97     24795\n","   macro avg       0.97      0.97      0.97     24795\n","weighted avg       0.97      0.97      0.97     24795\n","\n","\n","ğŸ“Œ Confusion Matrix (Original Test - Adv Model):\n"," [[12556   298]\n"," [  403 11538]]\n"]}]}]}